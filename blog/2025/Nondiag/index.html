<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <hr> <p>layout: post title: Scaling and Dimensionality Reduction: Understanding Non-diagonalizable Matrices date: 2023-04-09 tags: Statistics Mathematics categories: Notes citation: true —</p> <p>Geometrically, a matrix represents a linear mapping. This article aims to explore the geometric meaning of a general non-diagonalizable matrix, providing an intuitive explanation from the perspective of linear transformations.</p> <p>Here’s the situation: I encountered the following proof:</p> <blockquote> <p><strong>Proof:</strong> For a matrix $A \in \mathbb{R}^{m \times m}$, if all its eigenvalues have absolute values less than 1, then for any $x \in \mathbb{R}^{m}$, we have \(\lim_{k \to \infty} \lVert A^{k}x \rVert = 0.\)</p> </blockquote> <p>This proposition is very intuitive. The geometric meaning of the eigenvalues is that the matrix (or linear operator) scales in the directions of its eigenvectors; if the scaling factor in every direction is less than 1, then repeatedly applying the matrix will eventually compress any vector to 0.</p> <p>If the matrix is diagonalizable, there is a very straightforward geometric proof:</p> <p>Since the eigenvectors $v_{1}, \dots, v_{m}$ of a diagonalizable matrix $A$, with $Av_{i} = \lambda_{i}v_{i}$, form a basis of $\mathbb{R}^{m}$, any $x \in \mathbb{R}^{m}$ can be expressed as a linear combination of these eigenvectors: \(x = a_{1}v_{1} + \dots + a_{m}v_{m}, \quad \text{for some } a_{i} \in \mathbb{R}.\) Then, $A^{k}x$ can be written as: \(A^{k}x = a_{1}A^{k}v_{1} + \dots + a_{m}A^{k}v_{m} = a_{1}\lambda_{1}^{k}v_{1} + \dots + a_{m}\lambda_{m}^{k}v_{m}.\) Since $\lambda_{1}, \dots, \lambda_{m}$ are all less than 1, it follows that \(\lim_{k \to \infty} A^{k}x = 0.\)</p> <p>Of course, the proof presented in class was even more straightforward:</p> <p>Since $A$ is diagonalizable, it can be written as $A = P^{-1}\Lambda P$, where $P$ is invertible and $\Lambda$ is a diagonal matrix whose diagonal entries are the eigenvalues of $A$. Because all the diagonal entries of $\Lambda$ are less than 1, we have \(\lim_{k \to \infty} \Lambda^{k} = 0.\) Naturally, since $A^{k} = P^{-1}\Lambda^{k}P$, it follows that \(\lim_{k \to \infty} A^{k} = 0.\)</p> <p>Following this idea, the proof for non-diagonalizable matrices is also quite direct. Although such matrices cannot be diagonalized, we can similarly utilize their Jordan form: $A = P^{-1}JP$, where $P$ is invertible and $J$ is a block-diagonal matrix:</p> \[J = \begin{pmatrix} J_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; J_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; J_l \\ \end{pmatrix}, \quad \text{where} \quad J_{i} = \begin{pmatrix} \lambda_{i} &amp; 1 &amp; &amp; 0 \\ &amp; \ddots &amp; \ddots &amp; \\ &amp; &amp; \ddots &amp; 1 \\ 0 &amp; &amp; &amp; \lambda_{i} \end{pmatrix}.\] <p>Here, $\lambda_{1}, \dots, \lambda_{l}$ are the eigenvalues of $A$.</p> <p>We can then express $J$ as the sum of a diagonal matrix and another special matrix, namely, $J = D + N$: \(D = \begin{pmatrix} \lambda_{1} &amp; 0 &amp; &amp; 0 \\ &amp; \lambda_{1} &amp; &amp; \\ &amp; &amp; \ddots &amp; 0 \\ 0 &amp; &amp; &amp; \lambda_{l} \end{pmatrix} \quad N = \begin{pmatrix} 0 &amp; 1 &amp; &amp; 0 \\ &amp; 0 &amp; \ddots &amp; \\ &amp; &amp; \ddots &amp; 1 \\ 0 &amp; &amp; &amp; 0 \end{pmatrix}.\) In $N$, only the entries on the superdiagonals of the Jordan blocks (i.e., in the upper right corners of each block) are possibly 1 (or 0), and all other entries are 0.</p> <p>Notice that $N^{m} = 0$ (in fact, $N$ is a <a href="https://en.wikipedia.org/wiki/Nilpotent" rel="external nofollow noopener" target="_blank">nilpotent</a> operator). Therefore, for sufficiently large $k$, when expanding $J^{k}$ as a polynomial, all terms involving powers of $N$ higher than $m$ will vanish. We obtain: $$ J^{k} = D^{k} + \begin{pmatrix} k <br> k-1 \end{pmatrix} D^{k-1}N</p> <ul> <li>\dots + \begin{pmatrix} k <br> k-m+1 \end{pmatrix} D^{k-m+1}N^{m-1}. \(Since $m$ is fixed and the diagonal entries of $D$ are all less than 1, we have\) \lim_{k \to \infty} J^{k} = 0. $$ Q.E.D.</li> </ul> <p>But what does this proof actually imply? How can we understand this proposition geometrically, and how can we interpret a non-diagonalizable matrix?</p> <p>First, please keep in mind: <strong>A matrix represents a <a href="https://en.wikipedia.org/wiki/Linear_map" rel="external nofollow noopener" target="_blank">linear transformation</a></strong>. Let us return to the Jordan decomposition of the matrix $A$: $A = P^{-1}JP$. Note that $P$ is an invertible matrix, which in this formula serves the role of <strong>choosing a new basis for the linear space $\mathbb{R}^{m}$</strong>. This expression implies that when we take the column vectors of $P$ as a basis for $\mathbb{R}^{m}$ (in fact, this basis consists of the <a href="https://en.wikipedia.org/wiki/Generalized_eigenvector" rel="external nofollow noopener" target="_blank">generalized eigenvectors</a> of $A$), the linear transformation corresponding to $A$ can be represented by a block-diagonal matrix (the Jordan form). We can say that $A$ and $J$ are <a href="https://en.wikipedia.org/wiki/Isomorphism" rel="external nofollow noopener" target="_blank">isomorphic</a>, meaning that to understand the structure of $A$, we only need to understand the structure of $J$. Thus, we have: \(A \cong J = D + N.\) Here, $D$ is a diagonal matrix, which implies that along the directions of $A$’s generalized eigenvectors, scaling occurs; $N$ is a nilpotent matrix, meaning that it reduces the dimensionality of vectors in $\mathbb{R}^{m}$, i.e., it projects them onto a linear subspace. (The reader can verify this on their own. Note that $N$ is not a projection matrix; applying $N$ to $Nv$ will further project it onto an even smaller subspace.)</p> <p>At this point, we have a geometric characterization of a non-diagonalizable matrix: scaling combined with dimensionality reduction. Thus, it is intuitively clear that any matrix with all eigenvalues having absolute values less than 1, when raised to a sufficiently high power, will eventually converge to 0.</p> <p>PS: This article is intended for readers who have only studied elementary linear algebra, so I have avoided using the full language of linear operators. With more advanced terminology, one can have a more natural understanding of this issue.</p> <p><strong>References</strong>:</p> <p><a href="https://linear.axler.net" rel="external nofollow noopener" target="_blank">Axler, S. (2015). <em>Linear Algebra Done Right</em>. Springer.</a></p> </body></html>