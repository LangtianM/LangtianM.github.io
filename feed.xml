<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://langtianm.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://langtianm.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-09T01:01:42+00:00</updated><id>https://langtianm.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Simulation with Copulas</title><link href="https://langtianm.github.io/blog/2025/Copula/" rel="alternate" type="text/html" title="Simulation with Copulas"/><published>2025-05-29T00:00:00+00:00</published><updated>2025-05-29T00:00:00+00:00</updated><id>https://langtianm.github.io/blog/2025/Copula</id><content type="html" xml:base="https://langtianm.github.io/blog/2025/Copula/"><![CDATA[<p>This is a short introduction to simulation with copulas.</p> <h1 id="definition--main-result">Definition &amp; Main Result</h1> <p>A $d$-dimensional copula $C:[0, 1]^{d}\to[0, 1]$ is a cumulative distribution function with uniform marginals. Consider a $d$-dimensional CDF, $F$, with marginals $F_{1}, \dots, F_{d}$. Then tehre exists a copula $C_{X}$ such that</p> \[F(x_{1}, \dots, x_{d}) = C_{X}(F_{1}(x_{1}), \dots, F_{d}(x_{d}))\] <p>for all $x_{i}\in[-\infty, +\infty]$ and $i=1, 2, .., d$. $C_{X}$ is called the copula of $X$.</p> <h1 id="parametric-copulas">Parametric Copulas</h1> <p>Parametric copulas are copulas that are fully specified by a finite number of parameters. The most common parametric copulas are the Gaussian copula and the Student-t copula. We will focus on the Gaussian copula in this post.</p> <h2 id="the-guassian-copula">The Guassian Copula</h2> <p>Let $X \sim \mathcal{N}_{d}(0, \Sigma)$, and let $P$ be the correlation matrix. Then the corresponding Guassian copula is defined as</p> \[C_{P}^{\text{Guass}}(u) := \Phi_{P}(\Phi^{-1}(u_{1}), \dots, \Phi^{-1}(u_{d}))\] <p>where $\Phi$ is the standard univariate normal CDF and $\Phi_{P}$ denotes the joint CDF of $X$.</p> <p><strong>Remark:</strong> Guassian copula is fully specified by a correlation matrix $P$. i.e. for any $Y\sim\mathcal{N}_{d}(0, \Sigma’)$, as long as $\text{Corr}(Y)=P$, $Y$ has the same copula as $X$.</p> <h2 id="simualting-the-guassian-copula">Simualting the Guassian Copula</h2> <p>Guassian Copula allows us to simulate data $Y \in \mathbb{R}^{d}$ with any given marginal distribution $F_{1}, \dots, F_{d}$ and correlation structure $P$ . The algorithm goes as follows:</p> <ol> <li>For any given covariance matrix, $\Sigma$, let $P$ be its corresponding correlation matrix.</li> <li>Compute the Cholesky decomposition $P = A^{T}A$</li> <li>Generate $Z \sim \mathcal{N}<em>{d} (0, I</em>{d})$</li> <li>Set $X = A^{T}Z$</li> <li>Return $U =(\Phi(X_{1}), \dots, \Phi(X_{d}))$ Then we can define $Y_{i} = F_{i}^{-1}(U_{i})$, and we have generated $Y\in\mathbb{R}^{d}$ with the desired marginal distribution $F_{1}, \dots, F_{d}$ and correlation matrix $P$.</li> </ol> <h2 id="example-cdo-pricing">Example: CDO pricing</h2> <p>An investment bank is structuring and pricing a collateralized debt obligation (CDO) made up of $d$ corporate issuer. Each issuer has its own one-year default probability, but defaults tend to correlates with each other because of shared economic and industry factors. The goal is to capture both individual default risk and their dependence when valuing the different tranches of the CDO.</p> <p>We treat the default event for the $i$ th issuer as a Bernoulli outcome $Y_{i} \sim \text{Bernoulli}(p_{i})$ and introduce a latent standard-normal default random variable $X \in \mathbb{R}^{d}$ representing how likely a issuer is going to default: $Y_{i} = \mathbf{1}{ X_{i}\leq \Phi^{-1}(p_{i}) }$. Now we want to simulate $Y$.</p> <ul> <li>Marginals: Estimate each firm’s one-year default probability $p_i$ from CDS spreads or ratings.</li> <li>Dependence: Build a correlation matrix $P$ for latent normals $X_i$, using historical equity returns, industry buckets, or market “base correlations.”</li> <li>Simulation: <ol> <li>Cholesky-decompose $P=A A^T$.</li> <li>For each trial, draw $Z \sim \mathcal{N}(0, I)$, set $X=A Z$, then $U_i=\Phi\left(X_i\right)$.</li> <li>Declare default: $Y_{i} = \mathbf{1}{ X_{i}\leq \Phi^{-1}(p_{i}) }$.</li> </ol> </li> <li>Tranche Valuation: Aggregate losses per path, assign them to equity/mezzanine/senior tranches, discount expected losses to get fair spreads.</li> <li>Stress \&amp; Calibrate: Adjust $P$ (“implied correlation”) until model tranche prices match market quotes; re-simulate under higher correlations for tail-risk analysis.</li> </ul> <p><strong>Reference:</strong> <a href="https://www.columbia.edu/~mh2078/QRM/Copulas.pdf">Martin Haugh(2016), An Introduction to Copulas</a></p>]]></content><author><name></name></author><category term="Notes"/><category term="Statistics"/><summary type="html"><![CDATA[This is a short introduction to simulation with copulas.]]></summary></entry><entry><title type="html">Deriving Maximum Likelihood Estimation from Information Theory</title><link href="https://langtianm.github.io/blog/2024/InfoMLE/" rel="alternate" type="text/html" title="Deriving Maximum Likelihood Estimation from Information Theory"/><published>2024-11-16T00:00:00+00:00</published><updated>2024-11-16T00:00:00+00:00</updated><id>https://langtianm.github.io/blog/2024/InfoMLE</id><content type="html" xml:base="https://langtianm.github.io/blog/2024/InfoMLE/"><![CDATA[<p>In fields such as statistics and machine learning, Maximum Likelihood Estimation (MLE) is one of the most commonly used methods for parameter estimation. There are many reasons why we favor MLE: beyond its intuitive appeal, it also has excellent mathematical properties such as consistency, invariance, and asymptotic normality. This article offers another perspective on understanding MLE: deriving it from information theory.</p> <p>Assume that the observed data $x_1, \dots, x_n$ comes from a distribution $X \sim p_\theta$, where $p_\theta$ is the probability density function (pdf) of the distribution, and $\theta$ is the parameter to be estimated. From the data, we can derive its <strong>empirical distribution</strong>, whose empirical cumulative density function can be defined as:</p> \[\text{Pr}(X \leq x) = \hat{F}(x) = \frac{1}{n} \sum_{i=1}^{n} \mathbb{1}(x = x_i)\] <p>where $\mathbb{1}$ is the indicator function. The empirical probability density function<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> is:</p> \[\hat{p}(x) = \frac{1}{n}\sum_{i=1}^n \delta(x = x_i)\] <p>where $\delta$ is the <a href="https://en.wikipedia.org/wiki/Dirac_delta_function">Dirac delta function</a>.</p> <p>We want the estimated distribution $p_\theta$ to align with the observed actual data as closely as possible; in other words, $p_\theta$ should be as similar as possible to the empirical distribution $\hat{p}$. But how can we measure the similarity between two distributions? Information theory provides a tool: the distance between distributions, known as the KL divergence (see my <a href="https://langtianm.github.io/blog/2023/MutualInfo/">Note on Mutual Information</a> for its definition and interpretation). To make $p_\theta$ as close as possible to $\hat{p}$, we minimize the KL divergence between these two distributions:</p> \[\begin{align*} D_{KL}(\hat{p} \mid\mid p_\theta) &amp; = \mathbb{E}_{X \sim \hat{p}} \left[ \log \frac{\hat{p}(X)}{p_\theta(X)} \right] \\ &amp; = \mathbb{E}_{X \sim \hat{p}} [\log \hat{p}(X)] - \mathbb{E}_{X \sim \hat{p}} [\log p_\theta(X)] \end{align*}\] <p>Note that $\mathbb{E}_{X \sim \hat{p}} [\log \hat{p}(X)]$ is independent of $\theta$, so we have:</p> \[\begin{align*} \min_\theta D_{KL}(\hat{p} \mid\mid p_\theta) &amp; \Leftrightarrow \max_\theta \mathbb{E}_{X \sim \hat{p}} [\log p_\theta(X)] \\ &amp; \Leftrightarrow \max_\theta \frac{1}{n} \sum_{i=1}^n \int \log p_\theta(x) \delta(x = x_i) \, dx \\ &amp; \Leftrightarrow \max_\theta \frac{1}{n} \sum_{i=1}^n \log p_\theta(x_i) \end{align*}\] <p>That is, we maximize the log-likelihood function. Thus, starting from minimizing the distance between the empirical distribution and the estimated distribution, we have derived Maximum Likelihood Estimation.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>This is a generalized function, not directly derived from the cumulative density function (cdf), but it satisfies the properties of a probability density function. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="Notes"/><category term="Statistics"/><category term="Machine_Learning"/><summary type="html"><![CDATA[In fields such as statistics and machine learning, Maximum Likelihood Estimation (MLE) is one of the most commonly used methods for parameter estimation. There are many reasons why we favor MLE: beyond its intuitive appeal, it also has excellent mathematical properties such as consistency, invariance, and asymptotic normality. This article offers another perspective on understanding MLE: deriving it from information theory.]]></summary></entry><entry><title type="html">Second-order Inquiries on Statistics</title><link href="https://langtianm.github.io/blog/2024/Second-order/" rel="alternate" type="text/html" title="Second-order Inquiries on Statistics"/><published>2024-11-09T00:00:00+00:00</published><updated>2024-11-09T00:00:00+00:00</updated><id>https://langtianm.github.io/blog/2024/Second-order</id><content type="html" xml:base="https://langtianm.github.io/blog/2024/Second-order/"><![CDATA[<h2 id="what-is-statistics-really">What Is Statistics, Really?<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></h2> <p>Wikipedia defines statistics as “the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data”. However, this definition is too operational and insufficiently general. If I were to define statistics, I would call it <strong>the study of inductive reasoning</strong>.</p> <p>Statistics occupies a unique position; it is neither like mathematics and logic, which fall under the category of <a href="https://en.wikipedia.org/wiki/Formal_science">formalsciences</a>, deriving new truths from existing true propositions through deductive reasoning, nor is it like the natural sciences, such as physics or chemistry, which use inductive reasoning to derive knowledge about the world from observations. From my perspective, statistics tries to answer a fundamental epistemological question: how can we preserve the validity of inductive reasoning? Statistics tries to find optimal “inductive strategies” with desirable “deductive qualities”, and that’s why we attach importance to aspects like unbiasedness, invariance, and asymptotic normality.</p> <p>The fundamental flaw of inductive reasoning is that it is not truth-preserving. As David Hume illustrated, no matter how many black crows we observe, we can never conclusively claim that all crows are black; induction cannot guarantee truth beyond what we have seen. However, astonishingly, statistics lets us make certain statements like “under some assumptions, we believe all crows are black with over 95% confidence.” From incomplete and uncertain data, we manage to arrive at conclusions that are rigorously defensible—-a philosophically extraordinary feat that reveals the significance of statistical reasoning</p> <p>Of course, this assurance comes at a price: statistical statements are always weaker than the original propositions. Universal claims become probabilistic, and exact statements become approximate. Yet by accepting this trade-off, we transform inexpressible questions into expressible ones, and metaphysical problems into mathematical ones.</p> <p>Moreover, statistical inference always relies on an unverifiable assumption that the data originates from “some underlying probability distribution.” Reflecting on this assumption leads to another interesting question in philosophy of science: the question of realism versus instrumentalism.</p> <h2 id="statistical-models-reality-or-mere-tools">Statistical Models: Reality or Mere Tools?</h2> <p>In the philosophy of science, there is a long-standing debate between realism and instrumentalism. Realism holds that scientific theories should reflect reality, while instrumentalism requires only that they provide explanations and predictions.</p> <p>For example, is gravity an actual force present in the objective world, or is it just a “model” physicists use to describe our world? Most people lean towards the former, though proving this philosophically far from straightforward. In contrast, many adopt an instrumentalist stance on quantum theory, caring little about whether quantum states are “physical realities” and treating the theory purely as a “predictive tool”–sumed up by the famous phrase, “shut up and calculate.”</p> <p>However, in statistics, few concern themselves with whether models represent any “physical reality.”, and among those who do, almost none hold a realist stance. Some may see normal distributions or linear models as the “real-world mechanisms” generating data. But no one believes random forests or neural networks “exist” in any ontological sense; they are simply tools for prediction.</p> <p>More strikingly, identical mathematical structures often emerge in statistics from entirely different views on reality. For example, principal component analysis (PCA) can arise from maximizing data variance, minimizing distances to a linear subspace; or even from game-theoretic derivations. Philosophically, what does it mean when the same structure emerges from disparate perspectives?</p> <p>Unlike the natural sciences, statistics is highly instrumentalist, and this abandonment of realism has led to models with remarkable predictive power, such as deep learning. George Box’s famous saying, “All models are wrong, but some are useful,” perfectly encapsulates this instrumentalist mindset.</p> <h2 id="concluding-remarks">Concluding Remarks</h2> <p>Our discussion reveals how closely statistics is intertwined with epistemology. By employing mathematics and models, statistics offers a framework for making sense of an objective world that we can never fully grasp with certainty. This profound link reminds us that statistics is not merely a technical tool, but an essential part of our ongoing quest to understand the limits of human knowledge.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>My mind has been greatly influenced by <a href="https://en.wikipedia.org/wiki/Ludwig_Wittgenstein">Ludwig Wittgenstein</a> and Analytical Philosophy. This article discusses my thought about statistics from a philosophical perspective. Actually, it is the philosophical significance of statistics that attracts me to this field. Also, <a href="https://www.bilibili.com/video/BV1TD421g7Y6/?spm_id_from=333.999.0.0&amp;vd_source=122f65726f0fe678830e7adff2d7c3ff">here</a> is a talk (in Chinese) I gave to the junior students at SUSTech about why I chose to study statistics. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="Thoughts"/><category term="Philosophy"/><category term="Statistics"/><summary type="html"><![CDATA[What Is Statistics, Really?1 My mind has been greatly influenced by Ludwig Wittgenstein and Analytical Philosophy. This article discusses my thought about statistics from a philosophical perspective. Actually, it is the philosophical significance of statistics that attracts me to this field. Also, here is a talk (in Chinese) I gave to the junior students at SUSTech about why I chose to study statistics. &#8617;]]></summary></entry><entry><title type="html">Notes</title><link href="https://langtianm.github.io/blog/2024/LectNotes/" rel="alternate" type="text/html" title="Notes"/><published>2024-11-09T00:00:00+00:00</published><updated>2024-11-09T00:00:00+00:00</updated><id>https://langtianm.github.io/blog/2024/LectNotes</id><content type="html" xml:base="https://langtianm.github.io/blog/2024/LectNotes/"><![CDATA[<p>I would like to share some of my Notes on different subjects here.</p> <h1 id="statistics">Statistics</h1> <ul> <li><a href="https://langtianm.github.io/assets/documents/stat850notes.pdf">Theory and Application of Regression and Analysis of Variance II</a> (STAT850@UW-Madison, 2025Spring, typed by <a href="https://liiistem.com">LiiiSTEM</a>/<a href="https://mogan.app">Mogan</a>, in progress)</li> <li><a href="https://langtianm.github.io/assets/documents/Math_Stat.pdf">Mathematical Statistics Lecture Note</a> (STAT709@UW-Madison, 2024Fall, with mindmap)</li> <li><a href="https://langtianm.github.io/assets/documents/TSA.pdf">Time Series Analysis Lecture Note</a> (@SUSTech, 2023Spring, with mindmap)</li> <li><a href="https://langtianm.github.io/assets/documents/Spectral_Clustering.pdf">A Note on Spectral Clustering</a> (2023Spring, typed by <a href="https://www.texmacs.org">TeXmacs</a>)</li> </ul> <h1 id="mathematics">Mathematics</h1> <ul> <li><a href="https://langtianm.github.io/assets/documents/Real_Analysis.pdf">Real Analysis Lecture Note</a> (@SUSTech, 2022Fall, with mindmap)</li> <li><a href="https://langtianm.github.io/assets/documents/Done_Right_Mindmap.pdf">Linear Algebra Done Right Lecture Note</a> (@SUSTech, 2021Fall, mindmap only, in Chinese)</li> </ul>]]></content><author><name></name></author><category term="Notes"/><category term="Statistics"/><category term="Mathematics"/><summary type="html"><![CDATA[I would like to share some of my Notes on different subjects here.]]></summary></entry><entry><title type="html">Scaling and Dimensionality Reduction - Understanding Non-diagonalizable Matrices</title><link href="https://langtianm.github.io/blog/2024/Nondiag/" rel="alternate" type="text/html" title="Scaling and Dimensionality Reduction - Understanding Non-diagonalizable Matrices"/><published>2024-04-06T00:00:00+00:00</published><updated>2024-04-06T00:00:00+00:00</updated><id>https://langtianm.github.io/blog/2024/Nondiag</id><content type="html" xml:base="https://langtianm.github.io/blog/2024/Nondiag/"><![CDATA[<p>Geometrically, a matrix represents a linear mapping. This article aims to explore the geometric meaning of a general non-diagonalizable matrix, providing an intuitive explanation from the perspective of linear transformations.</p> <p>Here’s the situation: I encountered the following question:</p> <blockquote> <p>For a matrix $A \in \mathbb{R}^{m \times m}$, if all its eigenvalues have absolute values less than 1, then for any $x \in \mathbb{R}^{m}$, we have</p> \[\lim_{k \to \infty} \lVert A^{k}x \rVert = 0.\] </blockquote> <p>This proposition is very intuitive. The geometric meaning of the eigenvalues is that the matrix (or linear operator) scales in the directions of its eigenvectors; if the scaling factor in every direction is less than 1, then repeatedly applying the matrix will eventually compress any vector to 0.</p> <p>If the matrix is diagonalizable, there is a very straightforward geometric proof:</p> <p>Since the eigenvectors $v_{1}, \dots, v_{m}$ of a diagonalizable matrix $A$, with $Av_{i} = \lambda_{i}v_{i}$, form a basis of $\mathbb{R}^{m}$, any $x \in \mathbb{R}^{m}$ can be expressed as a linear combination of these eigenvectors: \(x = a_{1}v_{1} + \dots + a_{m}v_{m}, \quad \text{for some } a_{i} \in \mathbb{R}.\) Then, $A^{k}x$ can be written as: \(A^{k}x = a_{1}A^{k}v_{1} + \dots + a_{m}A^{k}v_{m} = a_{1}\lambda_{1}^{k}v_{1} + \dots + a_{m}\lambda_{m}^{k}v_{m}.\) Since $\lambda_{1}, \dots, \lambda_{m}$ are all less than 1, it follows that \(\lim_{k \to \infty} A^{k}x = 0.\)</p> <p>We can also make a more straightforward by decomposing the matrix $A$.</p> <p>Since $A$ is diagonalizable, it can be written as $A = P^{-1}\Lambda P$, where $P$ is invertible and $\Lambda$ is a diagonal matrix whose diagonal entries are the eigenvalues of $A$. Because all the diagonal entries of $\Lambda$ are less than 1, we have \(\lim_{k \to \infty} \Lambda^{k} = 0.\) Naturally, since $A^{k} = P^{-1}\Lambda^{k}P$, it follows that \(\lim_{k \to \infty} A^{k} = 0.\)</p> <p>Following this idea, the proof for non-diagonalizable matrices is also quite direct. Although such matrices cannot be diagonalized, we can similarly utilize their Jordan form: $A = P^{-1}JP$, where $P$ is invertible and $J$ is a block-diagonal matrix:</p> \[J = \begin{pmatrix} J_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; J_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; J_l \\ \end{pmatrix}, \quad \text{where} \quad J_{i} = \begin{pmatrix} \lambda_{i} &amp; 1 &amp; &amp; 0 \\ &amp; \ddots &amp; \ddots &amp; \\ &amp; &amp; \ddots &amp; 1 \\ 0 &amp; &amp; &amp; \lambda_{i} \end{pmatrix}.\] <p>Here, $\lambda_{1}, \dots, \lambda_{l}$ are the eigenvalues of $A$.</p> <p>We can then express $J$ as the sum of a diagonal matrix and another special matrix, namely, $J = D + N$:</p> \[D = \begin{pmatrix} \lambda_{1} &amp; 0 &amp; &amp; 0 \\ &amp; \lambda_{1} &amp; &amp; \\ &amp; &amp; \ddots &amp; 0 \\ 0 &amp; &amp; &amp; \lambda_{l} \end{pmatrix} \quad N = \begin{pmatrix} 0 &amp; 1 &amp; &amp; 0 \\ &amp; 0 &amp; \ddots &amp; \\ &amp; &amp; \ddots &amp; 1 \\ 0 &amp; &amp; &amp; 0 \end{pmatrix}.\] <p>In $N$, only the entries on the superdiagonals of the Jordan blocks (i.e., in the upper right corners of each block) are possibly 1 (or 0), and all other entries are 0.</p> <p>Notice that $N^{m} = 0$ (in fact, $N$ is a <a href="https://en.wikipedia.org/wiki/Nilpotent">nilpotent</a> operator). Therefore, for sufficiently large $k$, when expanding $J^{k}$ as a polynomial, all terms involving powers of $N$ higher than $m$ will vanish. We obtain:</p> \[J^{k} = D^{k} + \begin{pmatrix} k \\ k-1 \end{pmatrix} D^{k-1}N + \dots + \begin{pmatrix} k \\ k-m+1 \end{pmatrix} D^{k-m+1}N^{m-1}.\] <p>Since $m$ is fixed and the diagonal entries of $D$ are all less than 1, we have</p> \[\lim_{k \to \infty} J^{k} = 0.\] <p>Q.E.D.</p> <p>But what does this proof actually imply? How can we understand this proposition geometrically, and how can we interpret a non-diagonalizable matrix?</p> <p>First, please keep in mind: <strong>A matrix represents a <a href="https://en.wikipedia.org/wiki/Linear_map">linear transformation</a></strong>. Let us return to the Jordan decomposition of the matrix $A$: $A = P^{-1}JP$. Note that $P$ is an invertible matrix, which in this formula serves the role of <strong>choosing a new basis for the linear space $\mathbb{R}^{m}$</strong>. This expression implies that when we take the column vectors of $P$ as a basis for $\mathbb{R}^{m}$ (in fact, this basis consists of the <a href="https://en.wikipedia.org/wiki/Generalized_eigenvector">generalized eigenvectors</a> of $A$), the linear transformation corresponding to $A$ can be represented by a block-diagonal matrix (the Jordan form). We can say that $A$ and $J$ are <a href="https://en.wikipedia.org/wiki/Isomorphism">isomorphic</a>, meaning that to understand the structure of $A$, we only need to understand the structure of $J$. Thus, we have:</p> \[A \cong J = D + N.\] <p>Here, $D$ is a diagonal matrix, which implies that along the directions of $A$’s generalized eigenvectors, scaling occurs; $N$ is a nilpotent matrix, meaning that it reduces the dimensionality of vectors in $\mathbb{R}^{m}$, i.e., it projects them onto a linear subspace. (The reader can verify this on their own. Note that $N$ is not a projection matrix; applying $N$ to $Nv$ will further project it onto an even smaller subspace.)</p> <p>At this point, we have a geometric characterization of a non-diagonalizable matrix: scaling combined with dimensionality reduction. Thus, it is intuitively clear that any matrix with all eigenvalues having absolute values less than 1, when raised to a sufficiently high power, will eventually converge to 0.</p> <p>PS: This article is intended for readers who have only studied elementary linear algebra, so I have avoided using the full language of linear operators. With more advanced terminology, one can have a more natural understanding of this issue.</p> <p><strong>References</strong>:</p> <p><a href="https://linear.axler.net">Axler, S. (2015). <em>Linear Algebra Done Right</em>. Springer.</a></p>]]></content><author><name></name></author><category term="Notes"/><category term="Statistics"/><category term="Mathematics"/><summary type="html"><![CDATA[Geometrically, a matrix represents a linear mapping. This article aims to explore the geometric meaning of a general non-diagonalizable matrix, providing an intuitive explanation from the perspective of linear transformations.]]></summary></entry><entry><title type="html">A Note on Mutual Information</title><link href="https://langtianm.github.io/blog/2023/MutualInfo/" rel="alternate" type="text/html" title="A Note on Mutual Information"/><published>2023-04-09T00:00:00+00:00</published><updated>2023-04-09T00:00:00+00:00</updated><id>https://langtianm.github.io/blog/2023/MutualInfo</id><content type="html" xml:base="https://langtianm.github.io/blog/2023/MutualInfo/"><![CDATA[<blockquote> <p>This is a personal note I took a year ago when I was learning about the concept of Mutual Information for the first time. It was my initial encounter with information theory, and the materials I found didn’t provide clear definitions of the notations explicitly, which made it a bit challenging for me to follow. I rewrote everything in slightly more rigorous language, and in the end, I managed to understand it thoroughly from start to finish. Here, I’m sharing those notes from back then.</p> </blockquote> <h2 id="kullbackleibler-divergence">Kullback–Leibler Divergence</h2> <blockquote> <p>Notation: $\log$ refers to $\log_{2}$.</p> </blockquote> <p>A type of statistical distance to measure how one probability distribution $P$ is different from a second, reference probability distribution $Q$, denoted by $D_{KL}(P \mid\mid Q)$.</p> <p><strong>Definition</strong>: For probability distributions $P$ and $Q$ defined on the same sample space $\mathcal{X}$ , let $X\sim P$, the Kullback–Leibler divergence (relative entropy) <strong>from $Q$ to $P$</strong> is defined as:</p> \[D_{KL}(P\mid\mid Q) = \mathbb{E}\left[ \log\left( \frac{P(X)}{Q(X)} \right) \right]\] <p>This is only defined in the way if $Q(x) = 0$ implies $P(x) = 0$. When $P(x) = 0$, the corresponding term is interpreted as $0$ because $\lim_{ x \to 0^{+} }x\log(x)=0$.</p> <ul> <li> <p>Interpretation 1 :</p> \[D_{KL}(P\mid\mid Q)= \mathbb{E}[\log(P(X) - \log Q(X))]\] <p>which is the expected difference in log-likelihood</p> </li> <li> <p>Interpretation 2:</p> \[D_{KL}(P \mid\mid Q) = H(P, Q) - H(P)\] </li> </ul> <p>which is the difference of cross entropy of $(P, Q)$ and the entropy of $P$</p> <p><strong>Definition</strong> (Entropy) The entropy of a random variable $X$ is defined as:</p> \[H(X) = \mathbb{E}[-\log p(X)]\] <p><strong>Definition</strong> (Cross entropy) The cross entropy of the distribution $Q$ relative to a distribution $P$ is defined as:</p> \[H(P, Q) = -\mathbb{E}_{X \sim P}[\log Q(X)]\] <p>where $\mathbb{E}_{P}$ is the expected value operator with respect to the distribution $P$.</p> <h2 id="mutual-information">Mutual Information</h2> <p>The mutual information of two random variable is a measure of the mutual dependence between the two variables. It quantifies the “amount of information” obtained about one random variable by observing the other random variable.</p> <p><strong>Definition</strong>: Let $(X, Y)$ be a pair of random variables over the space $\mathcal{X}\times \mathcal{Y}$ . If their joint distribution is $P_{(X, Y)}$ and the marginal distributions are $P_{X}$ and $P_{Y}$, the mutual information is defined as</p> \[I(X;Y) = D_{KL}(P_{(X, Y)}\mid\mid P_{X} P_{Y}) = \mathbb{E}_{Y}[D_{KL}(P_{(X|Y)}\mid\mid P_{X})].\] <p>It is a measure of the price for encoding $(X, Y)$ as a pair of independent random variables when in reality they are not.</p> <p>Note that MI is more general than correlation coefficient, which can only depict the linear relationship.</p> <h2 id="evaluation-of-clustering-results">Evaluation of Clustering Results</h2> <p>Let $X$ be a random variable indicating the label of a data point given by a clustering algorithm, $Y$ be a random variable indicating the true label of a data point. Then we can calculate their sample mutual information by:</p> \[I(X, Y) = \sum_{i=1}^{k}\sum_{j=1}^{k}{\frac{|X_i\cap Y_j|}{N} \log_2\frac{N|X_i\cap Y_j|}{|X_i|\cdot |Y_j|}}\] <p>where $|X_i\cap Y_j|$ denote the number of data points with true label $j$ and be assigned with label $i$. The normalized mutual information is:</p> \[NMI(X, Y) = \frac{2I(X, Y)}{H(X)+H(Y)},\] <p>where $H(X)$, $H(Y)$ are entropies of $X$ and $Y$.</p>]]></content><author><name></name></author><category term="Notes"/><category term="Statistics"/><category term="Information_Theory"/><category term="Machine_Learning"/><summary type="html"><![CDATA[This is a personal note I took a year ago when I was learning about the concept of Mutual Information for the first time. It was my initial encounter with information theory, and the materials I found didn’t provide clear definitions of the notations explicitly, which made it a bit challenging for me to follow. I rewrote everything in slightly more rigorous language, and in the end, I managed to understand it thoroughly from start to finish. Here, I’m sharing those notes from back then.]]></summary></entry></feed>