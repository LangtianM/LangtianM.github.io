<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://langtianm.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://langtianm.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-10T05:43:16+00:00</updated><id>https://langtianm.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Second-order Inquiries on Statistics</title><link href="https://langtianm.github.io/blog/2024/Second-order/" rel="alternate" type="text/html" title="Second-order Inquiries on Statistics"/><published>2024-11-09T00:00:00+00:00</published><updated>2024-11-09T00:00:00+00:00</updated><id>https://langtianm.github.io/blog/2024/Second-order</id><content type="html" xml:base="https://langtianm.github.io/blog/2024/Second-order/"><![CDATA[<blockquote> <p>This article is originally written in Chinese. It was translated into English by ChatGPT and refined by the author. You can also see the Chinese version <a href="https://zhuanlan.zhihu.com/p/5990531684">here</a>.</p> </blockquote> <p>Wikipedia defines statistics as “the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data”. However, as a Ph.D. student in statistics and a philosophy enthusiast, I find this definition quite unsatisfactory. It is overly verbose, vague, and lacks precision. For instance, what does it mean to organize data? What does it mean to analyze it? By this definition, database principles should also fall under the umbrella of statistics, yet their methodologies differ significantly from what we recognize as statistical practice. Furthermore, is statistics truly a science? According to <a href="https://en.wikipedia.org/wiki/Karl_Popper">Karl Popper’s</a> criteria, statistics doesn’t quite fit the definition of a science. This realization has prompted me to examine what statistics truly is and to pose a series of deeper, second-order questions about its nature.</p> <p>Statistics occupies a unique position; it is neither like mathematics and logic, which fall under the category of <a href="[https://zhida.zhihu.com/search?content_id=650090079&amp;content_type=Answer&amp;match_order=1&amp;q=%E5%BD%A2%E5%BC%8F%E7%A7%91%E5%AD%A6&amp;zhida_source=entity](https://en.wikipedia.org/wiki/Formal_science)">formalsciences</a>, deriving new truths from existing true propositions through deductive reasoning, nor is it like the natural sciences, such as physics or chemistry, which use inductive reasoning to derive knowledge about the world from observations. From my perspective, statistics tries to answer a fundamental question in our knowledge: how can we preserve the va- lidity of inductive reasoning? If I were to define statistics, I would call it <strong>the study of inductive reasoning</strong>.</p> <p>Statistics tries to find optimal “inductive strategies” with desirable “deductive qualities”, and that’s why we attach importance to aspects such as unbiasedness, invariance, and asymptotic normality.</p> <p>Statistics is not a subset of mathematics, as many of its methods do not emerge solely from pure deductive reasoning. Its relationship with mathematics resembles that of physics with mathematics: both use mathematics as a tool to address problems within their own domains. But the research paradigm of statistics differs greatly from that of physics. Statistics tries to find optimal “inductive strategies” with desirable “deductive qualities”, and that’s why we attach importance to aspects such as unbiasedness, invariance, and asymptotic normality.</p> <p>The inherent unreliability of inductive reasoning is a fundamental flaw. Even if we observe a thousand black crows, we cannot conclusively state that “all crows are black.” However, using statistical tools, we can assert “with over 95% confidence that we believe all crows in the world are black.” We manage to draw reliable conclusions from inherently unreliable inductive data!</p> <p>Of course, this assurance comes at a price, as statistical inference is always based on an unverifiable assumption: that the data originates from some “probability distribution.” This raises an interesting question in the philosophy of science: realism or instrumentalism? Realism holds that scientific theories should reflect reality, while instrumentalism merely requires that they offer explanations and predictions. For example, is “gravity” an actual force present in the objective world, or is it just a model physicists use to describe our world? Most people would favor the former, though proving this philosophically is non-trivial. In contrast, many people adopt an instrumentalist approach to quantum theory, caring less about whether quantum states are “physical realities” and treating the theory as merely a “predictive tool”—in other words, “shut up and calculate.”</p> <p>In the case of statistics, few people concern themselves with whether statistical models represent any “physical reality.” Among those who do, hardly any hold a realist stance. Some may view normal distributions or linear models as the “real-world mechanisms” generating data. But no one believes random forests or neural networks “exist” in any ontological sense; they are simply tools for making predictions. Going further, we often derive identical mathematical structures in statistics from completely different views on reality. For example, principal component analysis (PCA) can be viewed as maximizing data variance or as minimizing the distance between data points and a linear subspace; it can even be derived from game theory (an astonishing coincidence—or is it?). What does it mean, philosophically, that identical structures emerge from different perspectives on reality? Unlike the natural sciences, statistics is highly instrumentalist, and this abandonment of realism has yielded models with remarkable predictive power, such as deep learning. George Box’s famous saying, “All models are wrong, but some are useful,” perfectly encapsulates this instrumentalist outlook.</p> <p>On a deeper level, many interesting questions arise. Since statistics is the study of inductive reasoning, it must be closely related to epistemology in philosophy. Statistics uses mathematics and models to construct a framework for understanding the objective world. In frequentist theory, we often assume there exist some underlying “true” parameters and models underlying the data, and our task is to use data to approximate this “truth.” This assertion, however, is open to question: Do these “true” parameters and models truly exist? What does “truth” mean? Is the objective world knowable? If so, to what extent? Readers familiar with philosophy will immediately recognize these as classic epistemological questions. In fact, such questions were thoroughly discussed and largely settled in Kant’s era, and these philosophical conclusions, in a certain sense, align well with the fundamental ideas in statistics.</p>]]></content><author><name></name></author><category term="Thoughts"/><category term="Philosophy"/><category term="Statistics"/><summary type="html"><![CDATA[This article is originally written in Chinese. It was translated into English by ChatGPT and refined by the author. You can also see the Chinese version here.]]></summary></entry><entry><title type="html">Lecture Notes</title><link href="https://langtianm.github.io/blog/2024/LectNotes/" rel="alternate" type="text/html" title="Lecture Notes"/><published>2024-11-09T00:00:00+00:00</published><updated>2024-11-09T00:00:00+00:00</updated><id>https://langtianm.github.io/blog/2024/LectNotes</id><content type="html" xml:base="https://langtianm.github.io/blog/2024/LectNotes/"><![CDATA[<p>I would like to share some of my handwritten lecture notes on different subjects here.</p> <h1 id="statistics">Statistics</h1> <ul> <li><a href="https://langtianm.github.io/assets/documents/Math_Stat.pdf">Mathematical Statistics</a> (STAT709@UW-Madison, 2024Fall, TBC)</li> <li><a href="https://langtianm.github.io/assets/documents/TSA.pdf">Time Series Analysis</a> (@SUSTech, 2023Spring, with mindmap)</li> </ul> <h1 id="mathematics">Mathematics</h1> <ul> <li><a href="https://langtianm.github.io/assets/documents/Real_Analysis.pdf">Real Analysis</a> (@SUSTech, 2022Fall, with mindmap)</li> <li><a href="https://langtianm.github.io/assets/documents/Done_Right_Mindmap.pdf">Linear Algebra Done Right</a> (@SUSTech, 2021Fall, mindmap only, in Chinese)</li> </ul>]]></content><author><name></name></author><category term="Notes"/><category term="Statistics"/><category term="Mathematics"/><summary type="html"><![CDATA[I would like to share some of my handwritten lecture notes on different subjects here.]]></summary></entry><entry><title type="html">A Note on Mutual Information</title><link href="https://langtianm.github.io/blog/2023/MutualInfo/" rel="alternate" type="text/html" title="A Note on Mutual Information"/><published>2023-04-09T00:00:00+00:00</published><updated>2023-04-09T00:00:00+00:00</updated><id>https://langtianm.github.io/blog/2023/MutualInfo</id><content type="html" xml:base="https://langtianm.github.io/blog/2023/MutualInfo/"><![CDATA[<blockquote> <p>This is a personal note I took a year ago when I was learning about the concept of Mutual Information for the first time. It was my initial encounter with information theory, and the materials I found didn’t provide clear definitions of the notations explicitly, which made it a bit challenging for me to follow. I rewrote everything in slightly more rigorous language, and in the end, I managed to understand it thoroughly from start to finish. Here, I’m sharing those notes from back then.</p> </blockquote> <h2 id="kullbackleibler-divergence">Kullback–Leibler Divergence</h2> <blockquote> <p>Note: $\log$ refers to $\log_{2}$ here.</p> </blockquote> <p>A type of statistical distance to measure how one probability distribution $P$ is different from a second, reference probability distribution $Q$, denoted by $D_{KL}(P \mid\mid Q)$.</p> <blockquote> <p><strong>Definition</strong>: For probability distributions $P$ and $Q$ defined on the same sample space $\mathcal{X}$ , let $X\sim P$, the Kullback–Leibler divergence (relative entropy) <strong>from $Q$ to $P$</strong> is defined as: \(D_{KL}(P\mid\mid Q) = \mathbb{E}\left[ \log\left( \frac{P(X)}{Q(X)} \right) \right]\)This is only defined in the way if $Q(x) = 0$ implies $P(x) = 0$. When $P(x) = 0$, the corresponding term is interpreted as $0$ because $\lim_{ x \to 0^{+} }x\log(x)=0$.</p> </blockquote> <ul> <li>Interpretation 1 : \(D_{KL}(P\mid\mid Q)= \mathbb{E}[\log(P(X) - \log Q(X))]\) which is the expected difference in log-likelihood</li> <li>Interpretation 2:\(D_{KL}(P \mid\mid Q) = H(P, Q) - H(P)\) which is the difference of cross entropy of $(P, Q)$ and the entropy of $P$</li> </ul> <blockquote> <p><strong>Definition</strong> (Entropy) The entropy of a random variable $X$ is defined as: \(H(X) = \mathbb{E}[-\log p(X)]\)</p> </blockquote> <blockquote> <p><strong>Definition</strong> (Cross entropy) The cross entropy of the distribution $Q$ relative to a distribution $P$ is defined as: \(H(P, Q) = -\mathbb{E}_{X \sim P}[\log Q(X)]\) where $\mathbb{E}_{P}$ is the expected value operator with respect to the distribution $P$.</p> </blockquote> <h2 id="mutual-information">Mutual Information</h2> <p>The mutual information of two random variable is a measure of the mutual dependence between the two variables. It quantifies the “amount of information” obtained about one random variable by observing the other random variable.</p> <blockquote> <p><strong>Definition</strong>: Let $(X, Y)$ be a pair of random variables over the space $\mathcal{X}\times \mathcal{Y}$ . If their joint distribution is $P_{(X, Y)}$ and the marginal distributions are $P_{X}$ and $P_{Y}$, the mutual information is defined as \(I(X;Y) = D_{KL}(P_{(X, Y)}\mid\mid P_{X} P_{Y}) = \mathbb{E}_{Y}[D_{KL}(P_{(X|Y)}\mid\mid P_{X})].\) It is a measure of the price for encoding $(X, Y)$ as a pair of independent random variables when in reality they are not.</p> </blockquote> <p>Note that MI is more general than correlation coefficient, which can only depict the linear relationship.</p> <h2 id="evaluation-of-clustering-results">Evaluation of Clustering Results</h2> <p>Let $X$ be a random variable indicating the label of a data point given by a clustering algorithm, $Y$ be a random variable indicating the true label of a data point. Then we can calculate their sample mutual information by: \(I(X, Y) = \sum_{i=1}^{k}\sum_{j=1}^{k}{\frac{|X_i\cap Y_j|}{N} \log_2\frac{N|X_i\cap Y_j|}{|X_i|\cdot |Y_j|}}\) where $|X_i\cap Y_j|$ denote the number of data points with true label $j$ and be assigned with label $i$. The normalized mutual information is: \(NMI(X, Y) = \frac{2I(X, Y)}{H(X)+H(Y)},\)where $H(X)$, $H(Y)$ are entropies of $X$ and $Y$.</p>]]></content><author><name></name></author><category term="Notes"/><category term="Statistics"/><category term="Information_Theory"/><category term="Machine_Learning"/><summary type="html"><![CDATA[This is a personal note I took a year ago when I was learning about the concept of Mutual Information for the first time. It was my initial encounter with information theory, and the materials I found didn’t provide clear definitions of the notations explicitly, which made it a bit challenging for me to follow. I rewrote everything in slightly more rigorous language, and in the end, I managed to understand it thoroughly from start to finish. Here, I’m sharing those notes from back then.]]></summary></entry></feed>